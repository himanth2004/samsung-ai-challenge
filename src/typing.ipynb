{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b84e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "# This is how you would have gotten the files' content as strings\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5274f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Attempt 1: Try a common encoding like 'latin-1' or 'cp1252'\n",
    "file_robot = pd.read_csv('human_text.txt', sep='\\t', encoding='latin-1')\n",
    "file_human = pd.read_csv('robot_text.txt', sep='\\t', encoding='latin-1')\n",
    "\n",
    "# Attempt 2: If the first attempt fails, try a different common encoding\n",
    "# file_robot = pd.read_csv('human_text.txt', sep='\\t', encoding='cp1252')\n",
    "# file_human = pd.read_csv('robot_text.txt', sep='\\t', encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ee863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same encoding that works for pandas\n",
    "with open('human_text.txt', 'r', encoding='latin-1') as f:\n",
    "    file_robot_content = f.read()\n",
    "\n",
    "with open('robot_text.txt', 'r', encoding='latin-1') as f:\n",
    "    file_human_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8801a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(txt):\n",
    "    lines = txt.split('\\n')\n",
    "    data = [{'text': line} for line in lines]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_robot = create_db(file_robot_content)\n",
    "df_human = create_db(file_human_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980cf03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_robot['human_v_robot'] = 0\n",
    "df_human['human_v_robot'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d836e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_robot, df_human])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4afc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>human_v_robot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh, thanks! I'm fine. this is an evening in my...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how do you feel today? tell me something about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how many virtual friends have you got?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is that forbidden for you to tell the exact nu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>nice !  i'm fine too</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>what means m ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>hi there !  !  how are you ?  ð</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>hi !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7091 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  human_v_robot\n",
       "0                                                                    0\n",
       "1     Oh, thanks! I'm fine. this is an evening in my...              0\n",
       "2     how do you feel today? tell me something about...              0\n",
       "3                how many virtual friends have you got?              0\n",
       "4     is that forbidden for you to tell the exact nu...              0\n",
       "...                                                 ...            ...\n",
       "2359                               nice !  i'm fine too              1\n",
       "2360                                    what means m ?               1\n",
       "2361                 hi there !  !  how are you ?  ð              1\n",
       "2362                                              hi !               1\n",
       "2363                                                                 1\n",
       "\n",
       "[7091 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cc8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d45dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e2eb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yourselves', \"mustn't\", 'about', 'just', \"won't\", \"mightn't\", 'yourself', \"she'll\", \"couldn't\", 'such', 'up', \"he'd\", \"should've\", 've', 'each', 'those', 'too', \"you'll\", 'below', 'all', \"haven't\", \"i'd\", 'what', 'before', 'hasn', 'its', 'off', 'our', \"they'd\", 'why', \"she's\", 'while', 'same', \"they're\", 'against', 're', 'into', 'i', 'myself', \"aren't\", 'shouldn', 'few', 'the', 'weren', 'me', 'it', \"shan't\", 'we', 'couldn', 'being', 'down', 'with', 'so', 'wouldn', 'this', \"we've\", 'during', 'has', 'didn', 'you', 'll', 'his', 'did', \"wasn't\", 'd', 'where', 'my', 'themselves', 'she', \"i've\", 'at', 's', 'through', \"he's\", \"didn't\", \"he'll\", \"needn't\", 'a', 'again', \"hadn't\", 'ourselves', 'been', 'that', 'above', 'after', \"it's\", 'theirs', 'ours', 'if', 'in', 'o', 'does', 'until', \"you've\", 'can', 'than', 'are', 'they', 'as', \"she'd\", 'when', 'm', 'shan', 'who', \"you're\", 'to', 'having', 'out', \"that'll\", 'any', 'further', 'ma', 'between', 'should', 'he', 'of', 'mustn', 'ain', \"they'll\", 'both', 'your', 'don', 'but', 'was', 'have', 'more', 'y', 'not', 'here', 'which', \"isn't\", \"shouldn't\", 'for', 'once', 'or', \"we're\", 'yours', 'aren', 'herself', \"i'm\", \"hasn't\", 'under', 'own', 'how', 'nor', \"it'll\", 'himself', \"weren't\", 'some', 'only', 't', 'will', 'won', 'is', 'hadn', 'these', 'over', \"we'll\", 'doesn', 'an', 'be', \"wouldn't\", 'by', 'am', 'had', 'whom', 'now', 'there', \"we'd\", 'doing', 'them', 'no', 'then', \"they've\", 'very', 'wasn', \"you'd\", 'her', 'itself', 'do', 'hers', 'their', \"doesn't\", 'most', 'isn', 'mightn', 'and', \"it'd\", 'on', 'needn', 'from', 'because', \"don't\", 'other', 'were', \"i'll\", 'haven', 'him'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee54ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"im\": \"i am\",\n",
    "    \"u\": \"you\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8816b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (5672,) (5672,)\n",
      "Test set size: (1419,) (1419,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ======== Define features and target ========\n",
    "X = df['text']           # Features: text column\n",
    "y = df['human_v_robot']  # Target: 0 = robot, 1 = human\n",
    "\n",
    "# ======== Train-test split ========\n",
    "# Use stratify=y to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% test data\n",
    "    random_state=42,    # for reproducibility\n",
    "    stratify=y          # maintain proportion of classes\n",
    ")\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(\"Training set size:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set size:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04afd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 78.58%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85       921\n",
      "           1       0.88      0.45      0.60       498\n",
      "\n",
      "    accuracy                           0.79      1419\n",
      "   macro avg       0.82      0.71      0.73      1419\n",
      "weighted avg       0.81      0.79      0.76      1419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Text preprocessing function\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    for key, val in replacements.items():\n",
    "        text = text.replace(key, val)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # remove punctuation\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 2. Convert text to features using TF-IDF (character n-grams work well for typing patterns\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2,4))\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['human_v_robot']\n",
    "\n",
    "# 3. Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Train Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy*100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01fd9a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from xgboost) (1.14.1)\n",
      "Downloading xgboost-3.0.4-py3-none-win_amd64.whl (56.8 MB)\n",
      "   ---------------------------------------- 0.0/56.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.1/56.8 MB 9.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 4.5/56.8 MB 10.3 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 6.8/56.8 MB 10.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 9.2/56.8 MB 10.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 11.5/56.8 MB 10.9 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 13.9/56.8 MB 11.0 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 16.3/56.8 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 18.9/56.8 MB 11.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 21.2/56.8 MB 11.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 23.6/56.8 MB 11.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 26.2/56.8 MB 11.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 28.6/56.8 MB 11.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 30.9/56.8 MB 11.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 33.3/56.8 MB 11.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 35.7/56.8 MB 11.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 38.3/56.8 MB 11.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 40.9/56.8 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 43.3/56.8 MB 11.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 45.6/56.8 MB 11.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 48.2/56.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 50.3/56.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 53.0/56.8 MB 11.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 55.3/56.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.8/56.8 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\DELL\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e73812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90f9a1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: [3781 1891]\n",
      "After SMOTE: [3781 3781]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [11:50:58] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[545 401]\n",
      " [ 57 416]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.58      0.70       946\n",
      "           1       0.51      0.88      0.64       473\n",
      "\n",
      "    accuracy                           0.68      1419\n",
      "   macro avg       0.71      0.73      0.67      1419\n",
      "weighted avg       0.77      0.68      0.68      1419\n",
      "\n",
      "ROC AUC Score: 0.8433014942184518\n"
     ]
    }
   ],
   "source": [
    "# ======== Imports ========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "\n",
    "# ======== Load your dataset ========\n",
    "# Replace this with your actual dataset\n",
    "# Assume X = features, y = target\n",
    "# Example: X = df.drop('target', axis=1), y = df['target']\n",
    "\n",
    "# X, y = ... (your dataset here)\n",
    "\n",
    "# ======== Train-test split ========\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ======== Handle class imbalance with SMOTE ========\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Before SMOTE: {np.bincount(y_train)}\")\n",
    "print(f\"After SMOTE: {np.bincount(y_res)}\")\n",
    "\n",
    "# ======== Train XGBoost classifier ========\n",
    "# Calculate scale_pos_weight for imbalance\n",
    "scale = len(y_res[y_res==0]) / len(y_res[y_res==1])\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_res, y_res)\n",
    "\n",
    "# ======== Make predictions ========\n",
    "# Get predicted probabilities\n",
    "y_probs = xgb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Adjust threshold to improve recall for minority class\n",
    "threshold = 0.3  # Lower than default 0.5\n",
    "y_pred = (y_probs >= threshold).astype(int)\n",
    "\n",
    "# ======== Evaluation ========\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dcea25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: [3781 1891]\n",
      "After SMOTE: [3781 2646]\n",
      "Optimal threshold: 0.4333497692252626\n",
      "Confusion Matrix:\n",
      " [[780 166]\n",
      " [ 84 389]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       946\n",
      "           1       0.70      0.82      0.76       473\n",
      "\n",
      "    accuracy                           0.82      1419\n",
      "   macro avg       0.80      0.82      0.81      1419\n",
      "weighted avg       0.84      0.82      0.83      1419\n",
      "\n",
      "Accuracy: 0.8238195912614518\n",
      "ROC AUC Score: 0.8869838063013735\n"
     ]
    }
   ],
   "source": [
    "# ======== Imports ========\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import xgboost as xgb\n",
    "import re\n",
    "\n",
    "# ======== Load your dataset ========\n",
    "# Assuming df already exists with columns: 'text' and 'human_v_robot'\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# ======== Text preprocessing ========\n",
    "# Contraction replacements\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    for k, v in replacements.items():\n",
    "        text = text.replace(k, v)\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "df['text'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# ======== Define features and target ========\n",
    "X = df['text']\n",
    "y = df['human_v_robot']\n",
    "\n",
    "# ======== Train-test split ========\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ======== TF-IDF Vectorization ========\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=5000,\n",
    "    ngram_range=(1,2)  # unigrams + bigrams\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# ======== Handle class imbalance with partial SMOTE ========\n",
    "smote = SMOTE(sampling_strategy=0.7, random_state=42)  # partial oversample\n",
    "X_res, y_res = smote.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "print(f\"Before SMOTE: {np.bincount(y_train)}\")\n",
    "print(f\"After SMOTE: {np.bincount(y_res)}\")\n",
    "\n",
    "# ======== Train XGBoost and RandomForest ========\n",
    "scale = len(y_res[y_res==0]) / len(y_res[y_res==1])\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ======== Voting Ensemble ========\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('rf', rf_model)],\n",
    "    voting='soft'  # use predicted probabilities\n",
    ")\n",
    "\n",
    "ensemble.fit(X_res, y_res)\n",
    "\n",
    "# ======== Predict probabilities ========\n",
    "y_probs = ensemble.predict_proba(X_test_vec)[:,1]\n",
    "\n",
    "# ======== Automatic Threshold Optimization ========\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-6)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(\"Optimal threshold:\", best_threshold)\n",
    "\n",
    "# Predict using optimized threshold\n",
    "y_pred_opt = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "# ======== Evaluation ========\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_opt))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_opt))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_opt))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d3e2b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['typing_model.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(ensemble, \"typing_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ee215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
